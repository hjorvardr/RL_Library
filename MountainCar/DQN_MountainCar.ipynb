{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "seed = 51\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "import gym\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# INSERT HERE THE NAME OF THE NETWORK. The FOLDER NAME should have this structure:\n",
    "# If I have a network with 2 layers, the first one with 10 neurons and the second one with 20 neurons, the file name is:\n",
    "# log_dir='./Monitoring/Test_10_20'\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./Monitoring/Test_5', histogram_freq=0, write_graph=True, write_images=True)\n",
    "# Note: pass in_keras=False to use this function with raw numbers of numpy arrays for testing\n",
    "def huber_loss(a, b, in_keras=True):\n",
    "    error = a - b\n",
    "    quadratic_term = error*error / 2\n",
    "    linear_term = abs(error) - 1/2\n",
    "    use_linear_term = (abs(error) > 1.0)\n",
    "    if in_keras:\n",
    "        # Keras won't let us multiply floats by booleans, so we explicitly cast the booleans to floats\n",
    "        use_linear_term = K.cast(use_linear_term, 'float32')\n",
    "    return use_linear_term * linear_term + (1-use_linear_term) * quadratic_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_size, output_size, learning_rate = 0.001, compile = True):\n",
    "    # Neural Net for Deep Q Learning\n",
    "    # Sequential() creates the foundation of the layers.\n",
    "    model = Sequential()\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 24 nodes\n",
    "    model.add(Dense(5, input_dim=input_size, activation='relu'))\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    model.add(Dense(output_size))\n",
    "    # Create the model based on the information above\n",
    "    if compile:\n",
    "        model.compile(loss='mean_squared_error', optimizer=RMSprop(lr=learning_rate))\n",
    "    # model.compile(loss=huber_loss, optimizer=Adam(lr=learning_rate))\n",
    "    # model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(sgd(lr=.2), \"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../statistics.py\"\n",
    "def ma(ts, q):\n",
    "    acc = 0\n",
    "    res = []\n",
    "    for i in range(q, len(ts) - q):\n",
    "        for j in range(i - q, i + q):\n",
    "            acc += ts[j]\n",
    "        res.append(acc / (2 * q + 1))\n",
    "        acc = 0\n",
    "    return res\n",
    "\n",
    "def accuracy(results):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of results, considering victories and defeats.\n",
    "    \"\"\"\n",
    "    return results[1] / (results[0]+results[1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../qlearning.py\"\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "\n",
    "def updateQ(Q, state, new_state, action, reward, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    state -> current state t\n",
    "    new_state -> next state t\n",
    "    reward -> reward\n",
    "    action -> current action\n",
    "    \"\"\"\n",
    "    future_action = np.argmax(Q[new_state]) # Find the best action to perform at time t+1\n",
    "    Q[state, action] = (1 - alpha)*Q[state, action] + alpha * (reward + gamma*Q[new_state, future_action])\n",
    "    return Q\n",
    "\n",
    "def updateQ_tensor(Q, state, new_state, action, reward, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule considering 3-dimensional matrices. It is used in MountainCar-v0 environment.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    state -> current state t\n",
    "    new_state -> next state t\n",
    "    reward -> reward\n",
    "    action -> current action\n",
    "    \"\"\"\n",
    "    future_action = np.argmax(Q[new_state[0],new_state[1]]) # Find the best action to perform at time t+1\n",
    "    Q[state[0],state[1], action] = (1 - alpha)*Q[state[0],state[1], action] + alpha * (reward + gamma*Q[new_state[0],new_state[1], future_action])\n",
    "    return Q\n",
    "\n",
    "def next_action1(state):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    max_value = np.amax(state)\n",
    "    max_indexes = np.arange(len(state))[state == max_value]\n",
    "    rn.shuffle(max_indexes)\n",
    "    return max_indexes[0]\n",
    "\n",
    "def next_action2(state,i_episode):\n",
    "    return np.argmax(state + np.random.randn(1,len(state))*(1./(i_episode+1)))\n",
    "\n",
    "def next_action3(state,epsilon):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    if np.random.uniform() > epsilon:\n",
    "        max_value = np.amax(state)\n",
    "        max_indexes = np.arange(len(state))[state == max_value]\n",
    "        rn.shuffle(max_indexes)\n",
    "        return max_indexes[0]\n",
    "    return np.argmax(np.random.uniform(0,1, size=4))\n",
    "\n",
    "def get_epsilon(k,n):\n",
    "    res = (n - k) / n\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_epsilon_exp(n):\n",
    "    res = 1 / (n + 1)\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import random as ran\n",
    "from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "#from keras.backend import stop_gradient\n",
    "\n",
    "def copy_weights(Q, Q_noob):\n",
    "    weights = Q.get_weights()\n",
    "    Q_noob.set_weights(weights)\n",
    "\n",
    "def replay(Q, random_pick, gamma, Q_noob):\n",
    "    Loss = []\n",
    "    Local_loss = []\n",
    "    for state, next_action, _reward, new_state, end in random_pick:\n",
    "        if not end:\n",
    "            _reward = _reward + gamma * np.amax(Q_noob.predict(new_state)[0])\n",
    "            #_reward = stop_gradient(_reward)\n",
    "\n",
    "        new_prediction = Q_noob.predict(state)\n",
    "        new_prediction[0][next_action] = _reward\n",
    "        Local_loss.append(Q.fit(state, new_prediction, epochs = 1, verbose = 0, callbacks = [tensorboard]).history['loss'][0])\n",
    "        Loss.append(np.mean(Local_loss))\n",
    "    return Loss\n",
    "           \n",
    "            \n",
    "def my_f(epsilon):\n",
    "    epsilon *= 0.95\n",
    "    epsilon = max(0.01, epsilon)\n",
    "    return epsilon\n",
    "    \n",
    "def experiment(alpha = 0.01, gamma = 0.5, n_episodes = 5000, max_action = 100000, final_pun = 0.5, step_pun = 0.07, default_policy = False, policy = np.zeros(64), render = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Execute an experiment given a configuration\n",
    "    Parameters:\n",
    "    alpha -> learning rate\n",
    "    gamma -> discount factor\n",
    "    n_episodes -> number of completed/failed plays\n",
    "    max_action -> maximum number of actions per episode\n",
    "    final_pun -> adjustment for the final reward\n",
    "    step_pun -> punishment for each step\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.device('/cpu:0'):\n",
    "        Res = [0,0] # array of results accumulator: {[0]: Loss, [1]: Victory}\n",
    "        Accuracy_res = [0,0]\n",
    "        Scores = [] # Cumulative rewards\n",
    "        Steps = [] # Steps per episode\n",
    "        Loss = []\n",
    "        Actions = []\n",
    "        epsilon = 0\n",
    "        \n",
    "        from gym import wrappers\n",
    "        from tqdm import tqdm\n",
    "        from collections import deque\n",
    "        env = gym.make('MountainCar-v0')\n",
    "        env._max_episode_steps = 1000000\n",
    "        # Set seeds\n",
    "        env.seed(seed)\n",
    "        #env = wrappers.Monitor(env, '/tmp/frozenlake-experiment-1', force=True)\n",
    "        Q = build_network(env.observation_space.shape[0], env.action_space.n)\n",
    "        Q_noob = build_network(env.observation_space.shape[0], env.action_space.n, compile=False)\n",
    "        memory = deque(maxlen = 3000)\n",
    "        batch_size = 32\n",
    "        for i_episode in tqdm(range(n_episodes), desc=\"Episode\"):\n",
    "            state = env.reset()\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            state = np.reshape(state,[1,2])\n",
    "            \n",
    "            t = 0\n",
    "            while True:\n",
    "            #for t in range(max_action):\n",
    "                if (t % 1000) == 0:\n",
    "                    print(\"t is:\",t)\n",
    "                if (render):\n",
    "                    env.render()\n",
    "                    #time.sleep(1)\n",
    "\n",
    "                if (default_policy):\n",
    "                    #if np.random.uniform() < 0.95:\n",
    "                    prediction = policy.predict(state)\n",
    "                    next_action = np.argmax(prediction[0])\n",
    "                    #else:\n",
    "                    #    next_action = np.argmax(np.random.uniform(0,1, size=3))\n",
    "                else:\n",
    "                    #epsilon = get_epsilon(i_episode, n_episodes)\n",
    "                    #epsilon = get_epsilon_exp(i_episode)\n",
    "                    #epsilon = my_f(epsilon)\n",
    "                    if t > 1000:\n",
    "                        #epsilon = epsilon + 0.0002 if epsilon < 0.9 else 0.9\n",
    "                        epsilon = epsilon + 0.0002 if epsilon < 1 else 1\n",
    "                        if t > 10000:\n",
    "                            epsilon = 0.9\n",
    "                    \n",
    "                    if np.random.uniform() < epsilon:\n",
    "                        prediction = Q.predict(state)\n",
    "                        next_action = np.argmax(prediction[0])\n",
    "                        Actions.append(next_action)\n",
    "                    else:\n",
    "                        next_action = np.argmax(np.random.uniform(0,1, size=3))\n",
    "                        #next_action = np.random.randint(0, 3)\n",
    "                        \n",
    "                new_state, reward, end, _ = env.step(next_action)\n",
    "\n",
    "                reward = abs(new_state[0] - (-0.5))     # r in [0, 1]\n",
    "                new_state = np.reshape(new_state,[1,2])\n",
    "                \n",
    "                memory.append((state, next_action, reward, new_state, end))\n",
    "\n",
    "                if end:\n",
    "                    if t == max_action -1:\n",
    "                        Res[0] += 1\n",
    "                    else:\n",
    "                        Res[1] += 1\n",
    "                        #print(\"ENTRATO!,\", t, \"steps\")\n",
    "\n",
    "                    Steps.append(t)\n",
    "                    break\n",
    "                else:\n",
    "                    state = new_state\n",
    "                    cumulative_reward += reward\n",
    "                \n",
    "                if t > 1000 and (t % 300) == 0 and default_policy == False:\n",
    "                    copy_weights(Q,Q_noob)\n",
    "                if t > 1000 and default_policy == False:   \n",
    "                    random_pick = ran.sample(memory, batch_size)\n",
    "                    Loss.append(replay(Q, random_pick, gamma, Q_noob))\n",
    "                t += 1\n",
    "\n",
    "            cumulative_reward += reward\n",
    "            Scores.append(cumulative_reward)\n",
    "        env.close()\n",
    "        return {\"results\": np.array(Res), \"steps\": np.array(Steps), \"scores\": np.array(Scores), \"Q\": Q, \"Q_noob\": Q_noob, \"loss\": np.array(Loss), \"actions\": np.array(Actions)}\n",
    "    \n",
    "    # TODO: documentare relazione tra RMSE e gradient descent optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .90, \"n_episodes\": 120, \"max_action\": 10000, \"final_pun\": 0, \"step_pun\": 0, \"render\": False}\n",
    "res = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save('model120episodesepsilonfix.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "trained_model = load_model('model500episodes_double_layer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scores\n",
    "#x = range(len(res[\"scores\"])-2*q)\n",
    "#plt.figure(figsize=(15,5))\n",
    "#plt.plot(x, ma(res[\"scores\"], q))\n",
    "#plt.errorbar(x, res[\"scores\"], fmt='ro', label=\"data\", xerr=0.75, ecolor='black')\n",
    "x = range(len(res[\"scores\"]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, res[\"scores\"])\n",
    "\n",
    "\n",
    "# Steps\n",
    "#x = range(len(res[\"steps\"])-2*q)\n",
    "#plt.figure(figsize=(15,5))\n",
    "#plt.plot(x, ma(res[\"steps\"],q))\n",
    "x = range(len(res[\"steps\"]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, res[\"steps\"])\n",
    "# Steps distribution\n",
    "plt.figure(figsize=(15,5))\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "plt.hist(res[\"steps\"],**kwargs)\n",
    "#plt.hist(res[\"steps\"], len(res[\"steps\"]), density=0, facecolor='green')\n",
    "\n",
    "# Steps distribution\n",
    "plt.figure(figsize=(15,5))\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "plt.hist(res[\"steps\"],**kwargs)\n",
    "#plt.hist(res[\"steps\"], len(res[\"steps\"]), density=0, facecolor='green')\n",
    "\n",
    "# Loss function\n",
    "x = range(len(res[\"loss\"]))\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.ylim(0,50)\n",
    "plt.plot(x, res[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .90, \"n_episodes\": 5, \"max_action\": 10000, \"final_pun\": 0, \"step_pun\": 0, \"default_policy\": True, \"policy\": trained_model, \"render\": True}\n",
    "res2 = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(res2[\"results\"]))\n",
    "print(np.mean(res2[\"scores\"]))\n",
    "print(res2[\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 2, \"max_action\": 200, \"final_pun\": 0, \"step_pun\": 0, \"default_policy\": True, \"policy\": learnt_policy, \"render\": True}\n",
    "res2 = experiment(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Bins  | Train Mean Score    | Test Mean Score | Accuracy |\n",
    "|-------|---------------------|-----------------|----------|\n",
    "| 70    | -166.78             | -152.97         | 98%      |\n",
    "| 80    | -162.06             | -147.19         | 100%     |\n",
    "| 90    | -158.856            | -130.3          | 100%     |\n",
    "| 100   | -158.567            | -169.68         | 100%     |\n",
    "| 130   | -162.172            | -132.06         | 100%     |\n",
    "| 150   | -169.692            | -129.28         | 100%     |\n",
    "| 180   | -179.890            | -141.28         | 100%     |\n",
    "| myalg | -198.66             | -244.71         | 26%      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
