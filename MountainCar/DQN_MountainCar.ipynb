{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_size,output_size,learning_rate = 0.001):\n",
    "    # Neural Net for Deep Q Learning\n",
    "    # Sequential() creates the foundation of the layers.\n",
    "    model = Sequential()\n",
    "    # 'Dense' is the basic form of a neural network layer\n",
    "    # Input Layer of state size(4) and Hidden Layer with 24 nodes\n",
    "    model.add(Dense(24, input_dim=input_size, activation='relu'))\n",
    "    # Hidden layer with 24 nodes\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    # Output Layer with # of actions: 2 nodes (left, right)\n",
    "    model.add(Dense(output_size, activation='linear'))\n",
    "    # Create the model based on the information above\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../statistics.py\"\n",
    "def ma(ts, q):\n",
    "    acc = 0\n",
    "    res = []\n",
    "    for i in range(q, len(ts) - q):\n",
    "        for j in range(i - q, i + q):\n",
    "            acc += ts[j]\n",
    "        res.append(acc / (2 * q + 1))\n",
    "        acc = 0\n",
    "    return res\n",
    "\n",
    "def accuracy(results):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of results, considering victories and defeats.\n",
    "    \"\"\"\n",
    "    return results[1] / (results[0]+results[1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../qlearning.py\"\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "\n",
    "def updateQ(Q, state, new_state, action, reward, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    state -> current state t\n",
    "    new_state -> next state t\n",
    "    reward -> reward\n",
    "    action -> current action\n",
    "    \"\"\"\n",
    "    future_action = np.argmax(Q[new_state]) # Find the best action to perform at time t+1\n",
    "    Q[state, action] = (1 - alpha)*Q[state, action] + alpha * (reward + gamma*Q[new_state, future_action])\n",
    "    return Q\n",
    "\n",
    "def updateQ_tensor(Q, state, new_state, action, reward, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule considering 3-dimensional matrices. It is used in MountainCar-v0 environment.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    state -> current state t\n",
    "    new_state -> next state t\n",
    "    reward -> reward\n",
    "    action -> current action\n",
    "    \"\"\"\n",
    "    future_action = np.argmax(Q[new_state[0],new_state[1]]) # Find the best action to perform at time t+1\n",
    "    Q[state[0],state[1], action] = (1 - alpha)*Q[state[0],state[1], action] + alpha * (reward + gamma*Q[new_state[0],new_state[1], future_action])\n",
    "    return Q\n",
    "\n",
    "def next_action1(state):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    max_value = np.amax(state)\n",
    "    max_indexes = np.arange(len(state))[state == max_value]\n",
    "    rn.shuffle(max_indexes)\n",
    "    return max_indexes[0]\n",
    "\n",
    "def next_action2(state,i_episode):\n",
    "    return np.argmax(state + np.random.randn(1,len(state))*(1./(i_episode+1)))\n",
    "\n",
    "def next_action3(state,epsilon):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    if np.random.uniform() > epsilon:\n",
    "        max_value = np.amax(state)\n",
    "        max_indexes = np.arange(len(state))[state == max_value]\n",
    "        rn.shuffle(max_indexes)\n",
    "        return max_indexes[0]\n",
    "    return np.argmax(np.random.uniform(0,1, size=4))\n",
    "\n",
    "def get_epsilon(k,n):\n",
    "    res = (n - k) / n\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_epsilon_exp(n):\n",
    "    res = 1 / (n + 1)\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import random as ran\n",
    "\n",
    "def experiment(alpha = 0.01, gamma = 0.5, n_episodes = 5000, max_action = 100000, final_pun = 0.5, step_pun = 0.07, default_policy = False, policy = np.zeros(64), render = False):\n",
    "    \"\"\"\n",
    "    Execute an experiment given a configuration\n",
    "    Parameters:\n",
    "    alpha -> learning rate\n",
    "    gamma -> discount factor\n",
    "    n_episodes -> number of completed/failed plays\n",
    "    max_action -> maximum number of actions per episode\n",
    "    final_pun -> adjustment for the final reward\n",
    "    step_pun -> punishment for each step\n",
    "    \"\"\"\n",
    "\n",
    "    Res = [0,0] # array of results accumulator: {[0]: Loss, [1]: Victory}\n",
    "    Accuracy_res = [0,0]\n",
    "    Scores = [] # Cumulative rewards\n",
    "    Steps = [] # Steps per episode\n",
    "    \n",
    "    from gym import wrappers\n",
    "    from tqdm import tqdm\n",
    "    from collections import deque\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env._max_episode_steps = max_action\n",
    "    # Set seeds\n",
    "    np.random.seed(88)\n",
    "    env.seed(88)\n",
    "     \n",
    "    #env = wrappers.Monitor(env, '/tmp/frozenlake-experiment-1', force=True)\n",
    "    Q = build_network(env.observation_space.shape[0], env.action_space.n)\n",
    "    memory = deque(maxlen = 200)\n",
    "    for i_episode in tqdm(range(n_episodes), desc=\"Episode\"):\n",
    "        state = env.reset()\n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        for t in range(max_action):\n",
    "            if (render):\n",
    "                env.render()\n",
    "                #time.sleep(1)\n",
    "            \n",
    "            if (default_policy):\n",
    "                prediction = policy.predict(np.reshape(state,[1,env.observation_space.shape[0]]))\n",
    "                next_action = np.argmax(prediction)\n",
    "            else:\n",
    "                epsilon = get_epsilon(i_episode, n_episodes)\n",
    "                #epsilon = get_epsilon_exp(i_episode)\n",
    "                prediction = Q.predict(np.reshape(state,[1,env.observation_space.shape[0]]))\n",
    "                if np.random.uniform() > epsilon:\n",
    "                    next_action = np.argmax(prediction)\n",
    "                else:\n",
    "                    next_action = np.argmax(np.random.uniform(0,1, size=3))\n",
    "            new_state, reward, end, info = env.step(next_action)\n",
    "            memory.append((state, next_action, reward, new_state, end))\n",
    "            \n",
    "            if end:\n",
    "                if t == max_action - 1:\n",
    "                    Res[0] += 1\n",
    "                else:\n",
    "                    Res[1] += 1\n",
    "                    \n",
    "                if reward == -1:\n",
    "                    reward = reward - final_pun\n",
    "                    \n",
    "                Steps.append(t)\n",
    "                break\n",
    "            else:\n",
    "                state = new_state\n",
    "                cumulative_reward += reward - step_pun\n",
    "            if (t + 1) % 32 == 0:\n",
    "                random_pick = ran.sample(memory, 32)\n",
    "                for state, next_action, reward, next_state, end in random_pick:\n",
    "                    if end:\n",
    "                        target = reward\n",
    "                    else:\n",
    "                        target = reward + gamma * np.amax(Q.predict(np.reshape(new_state,[1,env.observation_space.shape[0]])))\n",
    "                     \n",
    "                    prediction = Q.predict(np.reshape(state,[1,env.observation_space.shape[0]]))\n",
    "                    prediction[:,next_action] = target\n",
    "                    Q.fit(np.reshape(state,[1,env.observation_space.shape[0]]), prediction, epochs = 1, verbose = 0)\n",
    "                    \n",
    "        cumulative_reward += reward\n",
    "        Scores.append(cumulative_reward)\n",
    "    env.close()\n",
    "    return {\"results\": np.array(Res), \"steps\": np.array(Steps), \"scores\": np.array(Scores), \"Q\": Q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 1000, \"max_action\": 250, \"final_pun\": 0, \"step_pun\": 0, \"render\": False}\n",
    "res = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 20\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scores\n",
    "x = range(len(res[\"scores\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"scores\"], q))\n",
    "#plt.errorbar(x, res[\"scores\"], fmt='ro', label=\"data\", xerr=0.75, ecolor='black')\n",
    "\n",
    "# Steps\n",
    "x = range(len(res[\"steps\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"steps\"],q))\n",
    "\n",
    "# Steps distribution\n",
    "plt.figure(figsize=(15,5))\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "plt.hist(res[\"steps\"],**kwargs)\n",
    "#plt.hist(res[\"steps\"], len(res[\"steps\"]), density=0, facecolor='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 3, \"max_action\": 500, \"final_pun\": 0, \"step_pun\": 0, \"default_policy\": True, \"policy\": res[\"Q\"], \"render\": True}\n",
    "res2 = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(res2[\"results\"]))\n",
    "print(np.mean(res2[\"scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 2, \"max_action\": 200, \"final_pun\": 0, \"step_pun\": 0, \"default_policy\": True, \"policy\": learnt_policy, \"render\": True}\n",
    "res = experiment(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Bins  | Train Mean Score    | Test Mean Score | Accuracy |\n",
    "|-------|---------------------|-----------------|----------|\n",
    "| 70    | -166.78             | -152.97         | 98%      |\n",
    "| 80    | -162.06             | -147.19         | 100%     |\n",
    "| 90    | -158.856            | -130.3          | 100%     |\n",
    "| 100   | -158.567            | -169.68         | 100%     |\n",
    "| 130   | -162.172            | -132.06         | 100%     |\n",
    "| 150   | -169.692            | -129.28         | 100%     |\n",
    "| 180   | -179.890            | -141.28         | 100%     |\n",
    "| myalg | -198.66             | -244.71         | 26%      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
