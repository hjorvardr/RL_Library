{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../statistics.py\"\n",
    "def ma(ts, q):\n",
    "    acc = 0\n",
    "    res = []\n",
    "    for i in range(q, len(ts) - q):\n",
    "        for j in range(i - q, i + q):\n",
    "            acc += ts[j]\n",
    "        res.append(acc / (2 * q + 1))\n",
    "        acc = 0\n",
    "    return res\n",
    "\n",
    "def accuracy(results):\n",
    "    return results[1] / (results[0]+results[1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../qlearning.py\"\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "\n",
    "def updateQ(Q, state, new_state, action, reward, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    state -> current state t\n",
    "    new_state -> next state t\n",
    "    reward -> reward\n",
    "    action -> current action\n",
    "    \"\"\"\n",
    "    future_action = np.argmax(Q[new_state]) # Find the best action to perform at time t+1\n",
    "    Q[state, action] = (1 - alpha)*Q[state, action] + alpha * (reward + gamma*Q[new_state, future_action])\n",
    "    return Q\n",
    "\n",
    "def updateQ_tensor(Q, state, new_state, action, reward, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    state -> current state t\n",
    "    new_state -> next state t\n",
    "    reward -> reward\n",
    "    action -> current action\n",
    "    \"\"\"\n",
    "    future_action = np.argmax(Q[new_state[0],new_state[1]]) # Find the best action to perform at time t+1\n",
    "    Q[state[0],state[1], action] = (1 - alpha)*Q[state[0],state[1], action] + alpha * (reward + gamma*Q[new_state[0],new_state[1], future_action])\n",
    "    return Q\n",
    "\n",
    "def next_action1(state):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    max_value = np.amax(state)\n",
    "    max_indexes = np.arange(len(state))[state == max_value]\n",
    "    rn.shuffle(max_indexes)\n",
    "    return max_indexes[0]\n",
    "\n",
    "def next_action2(state,i_episode):\n",
    "    return np.argmax(state + np.random.randn(1,len(state))*(1./(i_episode+1)))\n",
    "\n",
    "def next_action3(state,epsilon):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    if np.random.uniform() > epsilon:\n",
    "        max_value = np.amax(state)\n",
    "        max_indexes = np.arange(len(state))[state == max_value]\n",
    "        rn.shuffle(max_indexes)\n",
    "        return max_indexes[0]\n",
    "    return np.argmax(np.random.uniform(0,1, size=4))\n",
    "\n",
    "def get_epsilon(k,n):\n",
    "    res = (n - k) / n\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_epsilon_exp(n):\n",
    "    res = 1 / (n + 1)\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "n_states = 10\n",
    "\n",
    "def obs_to_state(env, obs):\n",
    "    \"\"\" Maps an observation to state \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_dx = (env_high - env_low) / n_states\n",
    "    a = int((obs[0] - env_low[0])/env_dx[0])\n",
    "    b = int((obs[1] - env_low[1])/env_dx[1])\n",
    "    return a, b\n",
    "\n",
    "def experiment(alpha = 0.01, gamma = 0.5, n_episodes = 5000, max_action = 100000, final_pun = 0.5, step_pun = 0.07, default_policy = False, policy = np.zeros(64), render = False):\n",
    "    \"\"\"\n",
    "    Execute an experiment given a configuration\n",
    "    Parameters:\n",
    "    alpha -> learning rate\n",
    "    gamma -> discount factor\n",
    "    n_episodes -> number of completed/failed plays\n",
    "    max_action -> maximum number of actions per episode\n",
    "    final_pun -> adjustment for the final reward\n",
    "    step_pun -> punishment for each step\n",
    "    \"\"\"\n",
    "\n",
    "    Res = [0,0] # array of results accumulator: {[0]: Loss, [1]: Victory}\n",
    "    Accuracy_res = [0,0]\n",
    "    Scores = [] # Cumulative rewards\n",
    "    Steps = [] # Steps per episode\n",
    "    \n",
    "    from gym import wrappers\n",
    "    from tqdm import tqdm\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env._max_episode_steps = max_action\n",
    "    #env.tags['wrapper_config.TimeLimit.max_episode_steps'] = max_action\n",
    "    # Set seeds\n",
    "    np.random.seed(91)\n",
    "    env.seed(91)\n",
    "    \n",
    "    #max_velocity = int(round((env.observation_space.high[1] - env.observation_space.low[1]) * 100)) + 1\n",
    "    #max_position = int(round((env.observation_space.high[0] - env.observation_space.low[0]) * 100)) + 1\n",
    "    \n",
    "    #env = wrappers.Monitor(env, '/tmp/frozenlake-experiment-1', force=True)\n",
    "    #Q = np.zeros([max_position,max_velocity,env.action_space.n])\n",
    "    Q = np.zeros([n_states,n_states,env.action_space.n])\n",
    "    for i_episode in tqdm(range(n_episodes), desc=\"Episode\"):\n",
    "        state = env.reset()\n",
    "        #state = [int(round((state[0] - env.observation_space.low[0]) * 100)), int(round((state[1] - env.observation_space.low[1]) * 100))]\n",
    "        state = obs_to_state(env,state)\n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        for t in range(max_action):\n",
    "            if (render):\n",
    "                env.render()\n",
    "                #time.sleep(1)\n",
    "            \n",
    "            if (default_policy):\n",
    "                next_action = policy[state[0],state[1]]\n",
    "            else:\n",
    "                #epsilon = get_epsilon(i_episode, n_episodes)\n",
    "                epsilon = get_epsilon_exp(i_episode)\n",
    "                if np.random.uniform() > epsilon:\n",
    "                    next_action = next_action1(Q[state[0],state[1]])\n",
    "                else:\n",
    "                    next_action = np.argmax(np.random.uniform(0,1, size=3))\n",
    "            new_state, reward, end, info = env.step(next_action)\n",
    "            \n",
    "            #new_state = [int(round((new_state[0] - env.observation_space.low[0]) * 100)), int(round((new_state[1] - env.observation_space.low[1]) * 100))]\n",
    "            new_state = obs_to_state(env,new_state)\n",
    "            \n",
    "            if reward == -1:\n",
    "                Res[0] += 1\n",
    "            if reward == 0:\n",
    "                Res[1] += 1\n",
    "                \n",
    "            if end:\n",
    "                if reward == 0:\n",
    "                    reward = reward - final_pun\n",
    "                Q = updateQ_tensor(Q, state, new_state, next_action, reward, alpha, gamma)\n",
    "                Steps.append(t)\n",
    "                break\n",
    "            else:\n",
    "                Q = updateQ_tensor(Q, state, new_state, next_action, reward - step_pun, alpha, gamma)\n",
    "                state = new_state\n",
    "                cumulative_reward += reward - step_pun\n",
    "        cumulative_reward += reward\n",
    "        Scores.append(cumulative_reward)\n",
    "\n",
    "    env.close()\n",
    "    return {\"results\": np.array(Res), \"steps\": np.array(Steps), \"scores\": np.array(Scores), \"Q\": Q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 50, \"max_action\": 10000, \"final_pun\": 0, \"step_pun\": 0, \"render\": False}\n",
    "res = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scores\n",
    "x = range(len(res[\"scores\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"scores\"], q))\n",
    "#plt.errorbar(x, res[\"scores\"], fmt='ro', label=\"data\", xerr=0.75, ecolor='black')\n",
    "\n",
    "# Steps\n",
    "x = range(len(res[\"steps\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"steps\"],q))\n",
    "\n",
    "# Steps distribution\n",
    "plt.figure(figsize=(15,5))\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "plt.hist(res[\"steps\"],**kwargs)\n",
    "#plt.hist(res[\"steps\"], len(res[\"steps\"]), density=0, facecolor='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnt_policy = np.argmax(res[\"Q\"], axis=2)\n",
    "print(\"Policy learnt: \",learnt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(res[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 100, \"max_action\": 1000, \"final_pun\": 0, \"step_pun\": 0, \"default_policy\": True, \"policy\": learnt_policy, \"render\": False}\n",
    "res2 = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(res2[\"results\"]))\n",
    "print(np.mean(res2[\"scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 2, \"max_action\": 1000, \"final_pun\": 0, \"step_pun\": 0, \"default_policy\": True, \"policy\": learnt_policy, \"render\": True}\n",
    "res = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
