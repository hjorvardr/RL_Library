{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(alpha = .99, gamma = 0.1, n_episodes = 10000, max_action = 100):\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(73)\n",
    "\n",
    "    states = tf.placeholder(shape=[1,16], dtype=tf.float32)\n",
    "    weights = tf.Variable(tf.random_uniform([16,4],minval=0, maxval=0.01))\n",
    "    output = tf.matmul(states, weights)\n",
    "    action = tf.argmax(output, 1)\n",
    "\n",
    "    target_output = tf.placeholder(shape=[1,4], dtype=tf.float32)\n",
    "    loss = tf.reduce_sum(tf.square(target_output - output))\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    update = trainer.minimize(loss)\n",
    "\n",
    "    \n",
    "    Qs = [] # Weights matrices\n",
    "    Res = [] # Final results\n",
    "    Scores = [] # Cumulative rewards (Scores)\n",
    "    Steps = [] # Steps per episode\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for i in range(n_episodes):\n",
    "            s = env.reset()\n",
    "            acc_rew = 0\n",
    "            for j in range(max_action):\n",
    "                a = sess.run(action, feed_dict={states: np.identity(16)[s:s+1]})\n",
    "                Q = sess.run(output, feed_dict={states: np.identity(16)[s:s+1]})\n",
    "                s_new, reward, done, info = env.step(a[0])\n",
    "                Qnext = sess.run(output, feed_dict={states: np.identity(16)[s_new:s_new+1]})\n",
    "                amax = np.max(Qnext)\n",
    "                reward -= 0.1\n",
    "                Q[0,a[0]] = reward + gamma * amax\n",
    "                sess.run(update, feed_dict={states: np.identity(16)[s:s+1],target_output:Q})\n",
    "                if done == True:\n",
    "                    Res.append(reward)\n",
    "                    Steps.append(j)\n",
    "                    acc_rew += reward\n",
    "                    Scores.append(acc_rew)\n",
    "                    break\n",
    "                else:\n",
    "                    s = s_new\n",
    "                    acc_rew += reward\n",
    "    env.close()\n",
    "    return {\"results\": np.array(Res), \"steps\": np.array(Steps), \"scores\": np.array(Scores), \"Q\": weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../mylibrary.py\"\n",
    "def ma(ts, q):\n",
    "    acc = 0\n",
    "    res = []\n",
    "    for i in range(q, len(ts) - q):\n",
    "        for j in range(i - q, i + q):\n",
    "            acc += ts[j]\n",
    "        res.append(acc / (2 * q + 1))\n",
    "        acc = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 48\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scores\n",
    "x = range(len(res[\"scores\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"scores\"], q))\n",
    "\n",
    "# Steps\n",
    "x = range(len(res[\"scores\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"steps\"],q))\n",
    "\n",
    "# Steps\n",
    "x = range(np.max(res[\"steps\"]) + 1)\n",
    "plt.figure(figsize=(15,5))\n",
    "y = np.zeros(np.max(res[\"steps\"]) + 1)\n",
    "for i in range(len(res[\"steps\"])):\n",
    "    y[(res[\"steps\"][i])] += 1 \n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(res[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res[\"Q\"])\n",
    "print(np.argmax(res[\"Q\"],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
