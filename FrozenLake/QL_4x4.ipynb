{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_logging import log_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../statistics.py\"\n",
    "def ma(ts, q):\n",
    "    acc = 0\n",
    "    res = []\n",
    "    for i in range(q, len(ts) - q):\n",
    "        for j in range(i - q, i + q):\n",
    "            acc += ts[j]\n",
    "        res.append(acc / (2 * q + 1))\n",
    "        acc = 0\n",
    "    return res\n",
    "\n",
    "def accuracy(results):\n",
    "    return results[1] / (results[0]+results[1]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../qlearning.py\"\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "\n",
    "def updateQ(Q, state, new_state, action, reward, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    state -> current state t\n",
    "    new_state -> next state t\n",
    "    reward -> reward\n",
    "    action -> current action\n",
    "    \"\"\"\n",
    "    future_action = np.argmax(Q[new_state]) # Find the best action to perform at time t+1\n",
    "    Q[state, action] = (1 - alpha)*Q[state, action] + alpha * (reward + gamma*Q[new_state, future_action])\n",
    "    return Q\n",
    "\n",
    "def next_action1(state):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    max_value = np.amax(state)\n",
    "    max_indexes = np.arange(len(state))[state == max_value]\n",
    "    rn.shuffle(max_indexes)\n",
    "    return max_indexes[0]\n",
    "\n",
    "def next_action2(state,i_episode):\n",
    "    return np.argmax(state + np.random.randn(1,len(state))*(1./(i_episode+1)))\n",
    "\n",
    "def next_action3(state,epsilon):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    if np.random.uniform() > epsilon:\n",
    "        max_value = np.amax(state)\n",
    "        max_indexes = np.arange(len(state))[state == max_value]\n",
    "        rn.shuffle(max_indexes)\n",
    "        return max_indexes[0]\n",
    "    return np.argmax(np.random.uniform(0,1, size=4))\n",
    "\n",
    "def get_epsilon(k,n):\n",
    "    res = (n - k) / n\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_epsilon_exp(n):\n",
    "    res = 1 / (n + 1)\n",
    "    if res < 0.01:\n",
    "        return 0.01\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "def experiment(alpha = 0.01, gamma = 0.5, n_episodes = 5000, max_action = 100000, final_pun = 0.5, step_pun = 0.07, default_policy = False, policy = np.zeros(64), render = False):\n",
    "    \"\"\"\n",
    "    Execute an experiment given a configuration\n",
    "    Parameters:\n",
    "    alpha -> learning rate\n",
    "    gamma -> discount factor\n",
    "    n_episodes -> number of completed/failed plays\n",
    "    max_action -> maximum number of actions per episode\n",
    "    final_pun -> adjustment for the final reward\n",
    "    step_pun -> punishment for each step\n",
    "    \"\"\"\n",
    "\n",
    "    Res = [0,0] # array of results accumulator: {[0]: Loss, [1]: Victory}\n",
    "    Scores = [] # Cumulative rewards\n",
    "    Steps = [] # Steps per episode\n",
    "\n",
    "    from gym import wrappers\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    # Set seeds\n",
    "    np.random.seed(91)\n",
    "    env.seed(91)\n",
    "    \n",
    "    #env = wrappers.Monitor(env, '/tmp/frozenlake-experiment-1', force=True)\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    for i_episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        cumulative_reward = 0\n",
    "        \n",
    "        for t in range(max_action):\n",
    "            if (render):\n",
    "                env.render()\n",
    "                time.sleep(1)\n",
    "            \n",
    "            if (default_policy):\n",
    "                next_action = policy[state]\n",
    "            else:\n",
    "                epsilon = get_epsilon_exp(i_episode)\n",
    "                if np.random.uniform() > epsilon:\n",
    "                    next_action = next_action1(Q[state])\n",
    "                else:\n",
    "                    next_action = np.argmax(np.random.uniform(0,1, size=4))\n",
    "            new_state, reward, end, info = env.step(next_action)\n",
    "            if end:\n",
    "                Res[int(reward)] += 1\n",
    "                if reward == 0:\n",
    "                    reward = reward - final_pun\n",
    "                Q = updateQ(Q, state, new_state, next_action, reward, alpha, gamma)\n",
    "                Steps.append(t)\n",
    "                cumulative_reward += reward\n",
    "                Scores.append(cumulative_reward)\n",
    "                break\n",
    "            else:\n",
    "                Q = updateQ(Q, state, new_state, next_action, reward - step_pun, alpha, gamma)\n",
    "                state = new_state\n",
    "                cumulative_reward += reward - step_pun\n",
    "\n",
    "    env.close()\n",
    "    return {\"results\": np.array(Res), \"steps\": np.array(Steps), \"scores\": np.array(Scores), \"Q\": Q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 50000, \"max_action\": 100, \"final_pun\": 0, \"step_pun\": 0}\n",
    "res = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 100\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scores\n",
    "x = range(len(res[\"scores\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"scores\"], q))\n",
    "#plt.errorbar(x, res[\"scores\"], fmt='ro', label=\"data\", xerr=0.75, ecolor='black')\n",
    "\n",
    "# Steps\n",
    "x = range(len(res[\"steps\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"steps\"],q))\n",
    "\n",
    "# Steps distribution\n",
    "plt.figure(figsize=(15,5))\n",
    "kwargs = dict(histtype='stepfilled', alpha=0.3, density=True, bins=40)\n",
    "plt.hist(res[\"steps\"],**kwargs)\n",
    "#plt.hist(res[\"steps\"], len(res[\"steps\"]), density=0, facecolor='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learnt_policy = np.argmax(res[\"Q\"], axis=1)\n",
    "print(\"Policy learnt: \",learnt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(res[\"scores\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 5000, \"max_action\": 1000, \"final_pun\": 0, \"step_pun\": 0, \"default_policy\": True, \"policy\": learnt_policy}\n",
    "res = experiment(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy(res[\"results\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
