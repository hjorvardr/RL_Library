{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_logging import log_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../qlearning.py\"\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "\n",
    "def updateQ(Q, s_t, s_tn, a, R, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    s_tn -> new state\n",
    "    R -> reward\n",
    "    a -> action\n",
    "    \"\"\"\n",
    "    a_max = np.argmax(Q[s_tn])\n",
    "    Q[s_t, a] = (1 - alpha)*Q[s_t, a] + alpha * (R + gamma*Q[s_tn, a_max])\n",
    "    return Q\n",
    "\n",
    "def choose_policy(state):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    v_max = np.amax(state)\n",
    "    indexes = np.arange(len(state))[state == v_max]\n",
    "    rn.shuffle(indexes)\n",
    "    return indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(alpha = 0.01, gamma = 0.5, n_episodes = 5000, max_action = 100000, final_pun = 0.5, step_pun = 0.07):\n",
    "    Q = np.zeros((64, 4))\n",
    "    acc = 0;\n",
    "    Qs = [] # Weights matrices\n",
    "    Rs = [] # Final rewards\n",
    "    Crs = [] # Cumulative rewards\n",
    "    Ts = [] # Steps per episode\n",
    "\n",
    "    from gym import wrappers\n",
    "    env = gym.make('FrozenLake8x8-v0')\n",
    "    env = wrappers.Monitor(env, '/tmp/frozenlake-experiment-2', force=True)\n",
    "    for i_episode in range(n_episodes):\n",
    "        s_old = env.reset()\n",
    "        acc_rew = 0\n",
    "\n",
    "        for t in range(max_action):\n",
    "            #env.render()\n",
    "            policy = choose_policy(Q[s_old])\n",
    "            s_new, reward, done, info = env.step(policy)\n",
    "            if done:\n",
    "                Q = updateQ(Q, s_old, s_new, policy, reward - final_pun, alpha, gamma)\n",
    "                Qs.append(Q)\n",
    "                Rs.append(reward - final_pun)\n",
    "                Ts.append(t)\n",
    "                acc_rew += reward - final_pun\n",
    "                Crs.append(acc_rew)\n",
    "                break\n",
    "            else:\n",
    "                Q = updateQ(Q, s_old, s_new, policy, reward - step_pun, alpha, gamma)\n",
    "                s_old = s_new\n",
    "                acc_rew += reward - step_pun\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    return {\"final rewards\": np.array(Rs), \"steps\": np.array(Ts), \"cumulated rewards\": np.array(Crs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\"alpha\": 0.01, \"gamma\": 0.5, \"n_episodes\": 5000, \"max_action\": 100000, \"final_pun\": 0.5, \"step_pun\": 0.07},\n",
    "    {\"alpha\": 0.05, \"gamma\": 0.3, \"n_episodes\": 10000, \"max_action\": 100000, \"final_pun\": 0.5, \"step_pun\": 0.007},\n",
    "]\n",
    "\n",
    "#res = [experiment(**config) for config in configs]\n",
    "config = {\"alpha\": 0.01, \"gamma\": 0.5, \"n_episodes\": 5000, \"max_action\": 100000, \"final_pun\": 0.5, \"step_pun\": 0.07}\n",
    "res = []\n",
    "\n",
    "for alpha in np.arange(0.01, 0.99, 0.01):\n",
    "    config[\"alpha\"] = alpha\n",
    "    res.append(experiment(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0.01, 0.99, 0.01)\n",
    "reward_means = []\n",
    "for r in res:\n",
    "    reward_means.append(np.mean(r[\"final rewards\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(alphas, reward_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "for (i, r) in enumerate(res):\n",
    "    plt.plot(r[\"final rewards\"] / r[\"steps\"] + i * 0.)\n",
    "#plt.plot(res_1[\"final rewards\"] / res_1[\"steps\"] + 0.5, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "fit = Rs / Ts\n",
    "print(\"Rs %f Ts %f mFit %f\" % (Rs.sum(), Ts.sum(), fit.mean()))\n",
    "plt.plot(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "fit = Prs / Ts\n",
    "print(\"mFit %f\" % (fit.mean()))\n",
    "plt.plot(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy learned: {}\".format(np.argmax(Q, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "$\\gamma = 0.5,\\alpha = 0.01$\n",
    "\n",
    "[3 3 3 2 2 0 0 2 2 2 3 3 0 1 0 1 2 0 0 0 2 3 2 2 1 0 0 1 0 0 2 0 0 3 0 0 2\n",
    " 1 3 0 0 0 0 1 3 0 0 2 0 0 0 1 0 2 0 2 2 1 0 0 1 0 3 0] <- 5000 episodes  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
