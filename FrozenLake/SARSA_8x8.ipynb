{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook_logging import log_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../sarsa.py\"\n",
    "import numpy as np\n",
    "import numpy.random as rn\n",
    "\n",
    "def updateQ(Q, s_t, s_tn, a, a_n, R, alpha, gamma):\n",
    "    \"\"\"\n",
    "    It applies Q-Learning update rule.\n",
    "    Parameters:\n",
    "    Q -> Q matrix\n",
    "    s_tn -> new state\n",
    "    R -> reward\n",
    "    a -> action\n",
    "    \"\"\"\n",
    "    Q[s_t, a] = (1 - alpha) * Q[s_t, a] + alpha * (R + gamma*Q[s_tn, a_n])\n",
    "    return Q\n",
    "\n",
    "def choose_policy(state):\n",
    "    \"\"\"\n",
    "    It chooses the best action given the current state.\n",
    "    Paramteres:\n",
    "    state -> array of possible actions in the current state.\n",
    "    \"\"\"\n",
    "    v_max = np.amax(state)\n",
    "    indexes = np.arange(len(state))[state == v_max]\n",
    "    rn.shuffle(indexes)\n",
    "    return indexes[0]\n",
    "\n",
    "def choose_policy_greedy(state,env,i_episode):\n",
    "    return np.argmax(state + np.random.randn(1,env.action_space.n)*(1./(i_episode+1)))\n",
    "\n",
    "def gen_policy(Q):\n",
    "    return [choose_policy(state) for state in Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def experiment(alpha = 0.01, gamma = 0.5, n_episodes = 5000, max_action = 100000, final_pun = 0.5, step_pun = 0.07):\n",
    "    \"\"\"\n",
    "    Execute an experiment given a configuration\n",
    "    Parameters:\n",
    "    alpha -> learning rate\n",
    "    gamma -> discount factor\n",
    "    n_episodes -> number of completed/failed plays\n",
    "    max_action -> maximum number of actions per episode\n",
    "    final_pun -> adjustment for the final reward\n",
    "    step_pun -> punishment for each step\n",
    "    \"\"\"\n",
    "    # Q = np.zeros((64, 4))\n",
    "    Qs = [] # Weights matrices\n",
    "    Res = [] # Final results\n",
    "    Scores = [] # Cumulative rewards (Scores)\n",
    "    Steps = [] # Steps per episode\n",
    "    \n",
    "    won=0\n",
    "    \n",
    "    pol = [3, 2, 3, 2, 2, 3, 1, 1, 0, 0, 3, 2, 1, 0, 0, 2, 2, 0, 0, 0, 3, 2, 3, 2, 3, 1, 1, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0,\n",
    " 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "    from gym import wrappers\n",
    "    env = gym.make('FrozenLake8x8-v0')\n",
    "    np.random.seed(9)\n",
    "    env.seed(9)\n",
    "    #env = wrappers.Monitor(env, '/tmp/frozenlake-experiment-2', force=True)\n",
    "    Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    for i_episode in range(n_episodes):\n",
    "        s_old = env.reset()\n",
    "        acc_rew = 0\n",
    "        p = gen_policy(Q)\n",
    "        for t in range(max_action):\n",
    "            #env.render()\n",
    "            policy = p[s_old]\n",
    "            s_new, reward, done, info = env.step(policy)\n",
    "            if done:\n",
    "                if(reward==1):\n",
    "                    won+=1\n",
    "                reward = reward - final_pun\n",
    "                Q = updateQ(Q, s_old, s_new, policy, p[s_new], reward, alpha, gamma)\n",
    "                #Qs.append(Q) TODO: da togliere?\n",
    "                Res.append(reward)\n",
    "                Steps.append(t)\n",
    "                acc_rew += reward\n",
    "                Scores.append(acc_rew)\n",
    "                break\n",
    "            else:\n",
    "                Q = updateQ(Q, s_old, s_new, policy, p[s_new], reward - step_pun, alpha, gamma)\n",
    "                s_old = s_new\n",
    "                acc_rew += reward - step_pun\n",
    "\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return {\"results\": np.array(Res), \"steps\": np.array(Steps), \"scores\": np.array(Scores), \"Q\": Q, \"won\":won}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    {\"alpha\": 0.01, \"gamma\": 0.5, \"n_episodes\": 5000, \"max_action\": 100000, \"final_pun\": 0.5, \"step_pun\": 0.07},\n",
    "    {\"alpha\": 0.05, \"gamma\": 0.3, \"n_episodes\": 10000, \"max_action\": 100000, \"final_pun\": 0.5, \"step_pun\": 0.007},\n",
    "]\n",
    "\n",
    "np.random.seed(73)\n",
    "#config = {\"alpha\": 0.5, \"gamma\": 1, \"n_episodes\": 50000, \"max_action\": 10000, \"final_pun\": 0.5, \"step_pun\": 0.07}\n",
    "config = {\"alpha\": 0.8, \"gamma\": .95, \"n_episodes\": 10000, \"max_action\": 200, \"final_pun\": 0, \"step_pun\": 0}\n",
    "#res = [experiment(**config) for config in configs]\n",
    "res = experiment(**config)\n",
    "\n",
    "#for alpha in np.arange(0.01, 0.99, 0.01):\n",
    "#    config[\"alpha\"] = alpha\n",
    "#    res.append(experiment(**config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",res[\"won\"]/config[\"n_episodes\"])\n",
    "print(res[\"won\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../mylibrary.py\"\n",
    "def ma(ts, q):\n",
    "    acc = 0\n",
    "    res = []\n",
    "    for i in range(q, len(ts) - q):\n",
    "        for j in range(i - q, i + q):\n",
    "            acc += ts[j]\n",
    "        res.append(acc / (2 * q + 1))\n",
    "        acc = 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 48\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scores\n",
    "x = range(len(res[\"scores\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"scores\"], q))\n",
    "\n",
    "# Steps\n",
    "x = range(len(res[\"scores\"])-2*q)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(x, ma(res[\"steps\"],q))\n",
    "\n",
    "# Steps\n",
    "x = range(np.max(res[\"steps\"]) + 1)\n",
    "plt.figure(figsize=(15,5))\n",
    "y = np.zeros(np.max(res[\"steps\"]) + 1)\n",
    "for i in range(len(res[\"steps\"])):\n",
    "    y[(res[\"steps\"][i])] += 1 \n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy learned: {}\".format(np.argmax(res[\"Q\"], axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.arange(0.01, 0.99, 0.01)\n",
    "reward_means = []\n",
    "#for r in res:\n",
    "#    reward_means.append(np.mean(r[\"final rewards\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "$\\gamma = 1,\\alpha = 0.5$\n",
    "10k episodes\n",
    "\n",
    "[2 1 1 1 0 0 3 0 1 1 1 1 1 1 1 0 1 1 2 0 1 1 1 1 1 2 1 2 1 0 1 1 1 2 2 0 1\n",
    " 2 1 1 1 0 0 0 1 2 0 1 1 0 3 1 0 1 0 2 2 2 2 0 0 1 1 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
